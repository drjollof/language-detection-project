{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import seaborn as sns  \n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DATASET OVERVIEW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import dataset\n",
    "pd.set_option('display.max_rows', None)\n",
    "data = pd.read_csv('datasets\\\\language_dataset.csv')\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for the shape of dataset and missing values\n",
    "data.shape\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop unnecessary column\n",
    "data.drop(columns='Unnamed: 0', inplace=True)\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for number of observations per language\n",
    "data.replace('Portugese', 'Portuguese', inplace=True)\n",
    "data['language'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FEATURE EXTRACTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separate X and Y features\n",
    "x_data = data['Text']\n",
    "y_data = data['language']\n",
    "\n",
    "#split X and Y data with balanced number of observations in target variable\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data,\n",
    "                                                     test_size= 0.3, \n",
    "                                                     random_state=34, \n",
    "                                                     stratify = y_data\n",
    "                                                     )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#extract X features and check new shape\n",
    "vectorizer = CountVectorizer(analyzer = 'char' , ngram_range=(1,4))\n",
    "x_train_vectorized = vectorizer.fit_transform(x_train)\n",
    "x_train_vectorized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check vocabulary list\n",
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MODEL TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import all modules\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix,f1_score\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#specify model dictionary\n",
    "models = {\n",
    "     'Logistic Regression' : LogisticRegression(),\n",
    "     'Multinomial NB' : MultinomialNB(),\n",
    "     'SVC' : SVC(),\n",
    "     'RF Classifier' : RandomForestClassifier()\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train and evaluate train metrics\n",
    "print('TRAINING SET METRICS')\n",
    "for i in tqdm(range(len(list(models)))) :\n",
    "    #fit model\n",
    "    model_object = list(models.values())[i]\n",
    "    model = model_object.fit(x_train_vectorized, y_train)\n",
    "\n",
    "\n",
    "    #predict and collect metrics\n",
    "    y_train_predicted = model.predict(x_train_vectorized)\n",
    "\n",
    "\n",
    "    model_train_accuracy = accuracy_score(y_train, y_train_predicted)\n",
    "    model_train_precision = precision_score(y_train, y_train_predicted, average= \"weighted\")\n",
    "    model_train_recall = recall_score(y_train, y_train_predicted, average= \"weighted\")\n",
    "    model_train_f1_score = f1_score(y_train, y_train_predicted, average= \"weighted\" )\n",
    "\n",
    "\n",
    "    #print metrics\n",
    "    print(list(models.keys())[i])\n",
    "    print(f\"Accuracy : {model_train_accuracy:.4f}\")\n",
    "    print (f\"Precision: {model_train_precision:.4f}\")\n",
    "    print(f\"Recall : {model_train_recall:.4f}\")\n",
    "    print(f\"F1 score : {model_train_f1_score:.4f}\" )\n",
    "    print('=' *35)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train and evaluate test metrics\n",
    "print('TESTING SET METRICS')\n",
    "for i in tqdm(range(len(list(models)))) :\n",
    "    #fit model\n",
    "    model_object = list(models.values())[i]\n",
    "    model = model_object.fit(x_train_vectorized, y_train)\n",
    "\n",
    "\n",
    "    #predict and collect metrics\n",
    "    x_test_vectorized = vectorizer.transform(x_test)\n",
    "    y_test_predicted = model.predict(x_test_vectorized)\n",
    "\n",
    "\n",
    "    model_test_accuracy = accuracy_score(y_test, y_test_predicted)\n",
    "    model_test_precision = precision_score(y_test, y_test_predicted, average= \"weighted\")\n",
    "    model_test_recall = recall_score(y_test, y_test_predicted, average= \"weighted\")\n",
    "    model_test_f1_score = f1_score(y_test, y_test_predicted, average= \"weighted\" )\n",
    "\n",
    "    #print metrics\n",
    "    print(list(models.keys())[i])\n",
    "    print(f\"Accuracy : {model_test_accuracy:.4f}\")\n",
    "    print (f\"Precision: {model_test_precision:.4f}\")\n",
    "    print(f\"Recall : {model_test_recall:.4f}\")\n",
    "    print(f\"F1 score : {model_test_f1_score:.4f}\" )\n",
    "    print('=' *35)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TRAIN FINAL MODEL AND EVALUATE ON NEW DATASETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to plot confusion matrix\n",
    "def confusion(predicted_y, y):\n",
    "  plt.figure(figsize=(15,10))\n",
    "  languages = np.unique(y)\n",
    "  cm2 = confusion_matrix(y, predicted_y, labels= languages)\n",
    "  sns.heatmap(cm2, annot=True, fmt='d', cmap='Blues')\n",
    "  plt.xticks(np.arange(len(languages)) + 0.5, languages, rotation= 45)\n",
    "  plt.yticks(np.arange(len(languages)) + 0.5, languages, rotation = 360)\n",
    "  plt.tight_layout()\n",
    "  plt.xlabel('Predicted')\n",
    "  plt.ylabel('Actual')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train and evaluate MNB model\n",
    "MNB_object = MultinomialNB()\n",
    "MNB_model = MNB_object.fit(x_train_vectorized, y_train)\n",
    "x_test_vectorized = vectorizer.transform(x_test)\n",
    "mnb_predicted = MNB_model.predict(x_test_vectorized)\n",
    "mnb_accuracy = accuracy_score(y_test, mnb_predicted)\n",
    "print(f\"Accuracy : {mnb_accuracy:.4f}\")\n",
    "confusion(mnb_predicted, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to check the accuracy and confusion matrix of the model on a dataframe of text\n",
    "'''def predict_dataset_MNB(dataset):\n",
    "  new_test_data = None\n",
    "  for ext in ['.csv', '.txt']:\n",
    "    try:\n",
    "      new_test_data = pd.read_csv(\"datasets\\\\\" + dataset + ext,   delimiter= \",\") \n",
    "      break #exit loop if file is found\n",
    "    except FileNotFoundError:\n",
    "      pass #continue to check next extension\n",
    "  if new_test_data is None:   #if file is not found\n",
    "      print('File not found')\n",
    "      return #exit function\n",
    "  new_data_x = new_test_data['text']\n",
    "  new_data_y = new_test_data['language']\n",
    "  new_data_vectorized = vectorizer.transform(new_data_x)\n",
    "  new_predictions_NB = MNB_model.predict(new_data_vectorized)\n",
    "  accuracy = accuracy_score(new_data_y, new_predictions_NB)\n",
    "  print(f\"Accuracy : {accuracy:.4f}\")\n",
    "  confusion(new_predictions_NB, new_data_y) '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to predict a single text\n",
    "'''def predict_text_MNB(text):\n",
    "    text = vectorizer.transform([text])\n",
    "    prediction = MNB_model.predict(text)\n",
    "    print(prediction) '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use function to check new dataset metrics\n",
    "'''predict_dataset_MNB('multilingual-100')\n",
    "predict_dataset_MNB('multilingual-sentences-dataset')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load new dataset and check metrics\n",
    "new_data_2 = pd.read_csv('multilingual-sentences-dataset.txt', delimiter=',')\n",
    "new_data_2.head()\n",
    "new_data_2_x = new_data_2['text']\n",
    "new_data_2_y = new_data_2['language']\n",
    "new_data_2_x_vectorized = vectorizer.transform(new_data_2_x)\n",
    "new_data_2_x_vectorized.shape\n",
    "new_data_2_predicted = MNB_model.predict(new_data_2_x_vectorized)\n",
    "new_accuracy = accuracy_score(new_data_2_y, new_data_2_predicted)\n",
    "print(f\"Accuracy : {new_accuracy:.4f}\")\n",
    "confusion(new_data_2_predicted, new_data_2_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dataframe for misclassified languages\n",
    "new_df = pd.concat([new_data_2_x, new_data_2_y, pd.Series(new_data_2_predicted)], axis=1)\n",
    "new_df.columns = ['text','actual', 'predicted']\n",
    "errors_df = new_df[new_df['actual'] != new_df['predicted']]\n",
    "errors_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pickle files saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import joblib\n",
    "import gzip \n",
    "\n",
    "#save model and vectorizer as pickle files\n",
    "joblib.dump(MNB_model, os.path.join(os.getcwd(), \"model\", \"MNB_model.pkl\"), \n",
    "            compress= ('gzip', 3))\n",
    "joblib.dump(vectorizer, os.path.join(os.getcwd(), \"model\", \"vectorizer.pkl\"), \n",
    "            compress= ('gzip', 3))\n",
    "\n",
    "print(\"pickle files saved successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
